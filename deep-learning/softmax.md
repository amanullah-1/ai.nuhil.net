# Softmax

It is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes.

Softmax is an activation function that scales numbers/logits into probabilities. The output of a Softmax is a vector \(say `v`\) with probabilities of each possible outcome. The probabilities in vector `v` sums to one for all possible outcomes or classes.

{% embed url="https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78" %}



